{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Model\n",
    "import os\n",
    "import pandas as pd\n",
    "from shutil import copyfile\n",
    "import random\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\hackathon_1\n"
     ]
    }
   ],
   "source": [
    "working_dir = os.getcwd()\n",
    "print(working_dir)\n",
    "# dataframe for train.csv\n",
    "train_df = pd.read_csv(working_dir+'/train.csv')\n",
    "# dataframe for test.csv\n",
    "test_df = pd.read_csv(working_dir+'/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  image_names  emergency_or_not\n",
      "0    1503.jpg                 0\n",
      "1    1420.jpg                 0\n",
      "2    1764.jpg                 0\n",
      "3    1356.jpg                 0\n",
      "4    1117.jpg                 0\n",
      "  image_names\n",
      "0    1960.jpg\n",
      "1     668.jpg\n",
      "2    2082.jpg\n",
      "3     808.jpg\n",
      "4    1907.jpg\n"
     ]
    }
   ],
   "source": [
    "print(train_df.head())\n",
    "print(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2352\n"
     ]
    }
   ],
   "source": [
    "# 2352 total images\n",
    "print(len(os.listdir(working_dir+'/images')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G:\\hackathon_1/tmp failed\n",
      "G:\\hackathon_1/tmp/emer-v-nonemer failed\n",
      "G:\\hackathon_1/tmp/emer-v-nonemer/training failed\n",
      "G:\\hackathon_1/tmp/emer-v-nonemer/testing failed\n",
      "G:\\hackathon_1/tmp/emer-v-nonemer/training/emer failed\n",
      "G:\\hackathon_1/tmp/emer-v-nonemer/training/nonemer failed\n",
      "G:\\hackathon_1/tmp/emer-v-nonemer/testing/emer failed\n",
      "G:\\hackathon_1/tmp/emer-v-nonemer/testing/nonemer failed\n"
     ]
    }
   ],
   "source": [
    "# create folders\n",
    "to_create = [\n",
    "    working_dir+'/emer_source',\n",
    "    working_dir+'/nonemer_source',\n",
    "    working_dir+'/tmp',\n",
    "    working_dir+'/tmp/emer-v-nonemer',\n",
    "    working_dir+'/tmp/emer-v-nonemer/training',\n",
    "    working_dir+'/tmp/emer-v-nonemer/testing',\n",
    "    working_dir+'/tmp/emer-v-nonemer/training/emer',\n",
    "    working_dir+'/tmp/emer-v-nonemer/training/nonemer',\n",
    "    working_dir+'/tmp/emer-v-nonemer/testing/emer',\n",
    "    working_dir+'/tmp/emer-v-nonemer/testing/nonemer'\n",
    "]\n",
    "\n",
    "for directory in to_create:\n",
    "    try:\n",
    "        os.mkdir(directory)\n",
    "        print(directory, 'created')\n",
    "    except:\n",
    "        print(directory, 'failed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "emer_training = working_dir+'/tmp/emer-v-nonemer/training/emer/'\n",
    "nonemer_training = working_dir+'/tmp/emer-v-nonemer/training/nonemer/'\n",
    "emer_testing = working_dir+'/tmp/emer-v-nonemer/testing/emer/'\n",
    "nonemer_testing = working_dir+'/tmp/emer-v-nonemer/testing/nonemer/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1503.jpg\n",
      "0\n",
      "1960.jpg\n",
      "image_names         1503.jpg\n",
      "emergency_or_not           0\n",
      "Name: 0, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(train_df['image_names'][0])\n",
    "print(train_df['emergency_or_not'][0])\n",
    "print(test_df['image_names'][0])\n",
    "\n",
    "print(train_df.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate dataframes for emergency and nonemergency images in training set\n",
    "training_emer = train_df.loc[train_df['emergency_or_not']==0]\n",
    "training_nonemer = train_df.loc[train_df['emergency_or_not']==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Move the labelled files into a separate folder for training and validation files\n",
    "# for i in train_df['image_names']:\n",
    "#     image_name = i\n",
    "#     copyfile(working_dir+'/images/'+i, working_dir+'/trainValImages/'+i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in training_emer['image_names']:\n",
    "    image_name = i\n",
    "    copyfile(working_dir+'/images/'+i, working_dir+'/emer_source/'+i)\n",
    "    \n",
    "for i in training_nonemer['image_names']:\n",
    "    image_name = i\n",
    "    copyfile(working_dir+'/images/'+i, working_dir+'/nonemer_source/'+i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
    "    all_files = []\n",
    "    \n",
    "    for file_name in os.listdir(SOURCE):\n",
    "        file_path = SOURCE + file_name\n",
    "\n",
    "        if os.path.getsize(file_path):\n",
    "            all_files.append(file_name)\n",
    "        else:\n",
    "            print('{} is zero length, so ignoring'.format(file_name))\n",
    "    \n",
    "    n_files = len(all_files)\n",
    "    split_point = int(n_files * SPLIT_SIZE)\n",
    "    \n",
    "    shuffled = random.sample(all_files, n_files)\n",
    "    \n",
    "    train_set = shuffled[:split_point]\n",
    "    test_set = shuffled[split_point:]\n",
    "    \n",
    "    for file_name in train_set:\n",
    "        copyfile(SOURCE + file_name, TRAINING + file_name)\n",
    "        \n",
    "    for file_name in test_set:\n",
    "        copyfile(SOURCE + file_name, TESTING + file_name)\n",
    "\n",
    "emer_source_dir = (working_dir+'/emer_source/')\n",
    "train_emer_dir = emer_training\n",
    "test_emer_dir = emer_testing\n",
    "\n",
    "nonemer_source_dir = (working_dir+'/nonemer_source/')\n",
    "train_nonemer_dir = nonemer_training\n",
    "test_nonemer_dir = nonemer_testing\n",
    "\n",
    "split_size = .9\n",
    "split_data(emer_source_dir, train_emer_dir, test_emer_dir, split_size)\n",
    "split_data(nonemer_source_dir, train_nonemer_dir, test_nonemer_dir, split_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "965\n",
      "681\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(emer_source_dir)))\n",
    "print(len(os.listdir(nonemer_source_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "868\n",
      "97\n",
      "-----\n",
      "612\n",
      "69\n"
     ]
    }
   ],
   "source": [
    "print(len(os.listdir(train_emer_dir)))\n",
    "print(len(os.listdir(test_emer_dir)))\n",
    "print('-----')\n",
    "print(len(os.listdir(train_nonemer_dir)))\n",
    "print(len(os.listdir(test_nonemer_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "image = cv2.imread(working_dir+'/images/0.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2.imshow('title',image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(224, 224, 3)),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Conv2D(64, (2,2), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(2,2),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(512, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=RMSprop(lr=0.001), loss='binary_crossentropy', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1480 images belonging to 2 classes.\n",
      "Found 166 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "TRAINING_DIR = working_dir+'/tmp/emer-v-nonemer/training'\n",
    "train_datagen = ImageDataGenerator(rescale = 1 / 255)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    TRAINING_DIR,\n",
    "    batch_size=64,\n",
    "    class_mode='binary',\n",
    "    target_size=(224, 224)\n",
    ")\n",
    "\n",
    "VALIDATION_DIR = working_dir+'/tmp/emer-v-nonemer/testing'\n",
    "validation_datagen = ImageDataGenerator(rescale= 1/255)\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    VALIDATION_DIR,\n",
    "    batch_size=64,\n",
    "    class_mode='binary',\n",
    "    target_size=(224, 224)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warnings ignored\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "print('warnings ignored')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3319, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-53-5f243048186a>\", line 5, in <module>\n",
      "    validation_data=validation_generator\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 1297, in fit_generator\n",
      "    steps_name='steps_per_epoch')\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_generator.py\", line 265, in model_iteration\n",
      "    batch_outs = batch_function(*batch_data)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\", line 973, in train_on_batch\n",
      "    class_weight=class_weight, reset_metrics=reset_metrics)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\", line 264, in train_on_batch\n",
      "    output_loss_metrics=model._output_loss_metrics)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\", line 311, in train_on_batch\n",
      "    output_loss_metrics=output_loss_metrics))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_eager.py\", line 272, in _process_single_batch\n",
      "    model.optimizer.apply_gradients(zip(grads, trainable_weights))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\", line 434, in apply_gradients\n",
      "    self._create_hypers()\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\optimizer_v2\\optimizer_v2.py\", line 633, in _create_hypers\n",
      "    for name, value in sorted(self._hyper.items()):\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2034, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1151, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 319, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 353, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1502, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 1460, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\ProgramData\\Anaconda3\\lib\\inspect.py\", line 732, in getmodule\n",
      "    for modname, module in list(sys.modules.items()):\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=5,\n",
    "    verbose=1,\n",
    "    validation_data=validation_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
